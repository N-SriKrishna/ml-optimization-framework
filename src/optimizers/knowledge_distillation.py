"""
Knowledge Distillation - Compress models by transferring knowledge from teacher to student
"""
from pathlib import Path
import torch
import torch.nn as nn
import onnx
import onnxruntime as ort
from src.utils.logger import logger


class KnowledgeDistiller:
    """Distill knowledge from a large teacher model to a smaller student model"""
    
    def __init__(self, temperature=3.0, alpha=0.5):
        """
        Args:
            temperature: Softening factor for distillation
            alpha: Balance between distillation loss and ground truth loss
        """
        self.temperature = temperature
        self.alpha = alpha
    
    def distillation_loss(self, student_logits, teacher_logits, labels, temperature):
        """Calculate distillation loss"""
        soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=1)
        soft_prob = nn.functional.log_softmax(student_logits / temperature, dim=1)
        
        # Distillation loss
        soft_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size(0)
        
        # Hard loss (ground truth)
        hard_loss = nn.functional.cross_entropy(student_logits, labels)
        
        # Combined loss
        loss = self.alpha * (temperature ** 2) * soft_loss + (1 - self.alpha) * hard_loss
        return loss
    
    def create_student_model(self, teacher_model, compression_ratio=0.5):
        """
        Create a smaller student model based on teacher architecture
        
        Args:
            teacher_model: Original ONNX model
            compression_ratio: How much to compress (0.5 = 50% smaller)
        """
        # For ONNX models, we'll use quantization + pruning as "distillation"
        # True distillation requires training, which is beyond ONNX scope
        
        logger.info(f"Creating student model with {compression_ratio:.0%} compression")
        logger.warning("Note: Full distillation requires training. Using quantization as proxy.")
        
        return teacher_model


def apply_knowledge_distillation(
    teacher_path: str,
    student_path: str,
    compression_ratio: float = 0.5,
    output_path: str = None
) -> Path:
    """
    Apply knowledge distillation (simplified for ONNX)
    
    Args:
        teacher_path: Path to teacher ONNX model
        student_path: Path to student model (or None to create)
        compression_ratio: Target compression ratio
        output_path: Where to save distilled model
    
    Returns:
        Path to distilled model
    """
    if output_path is None:
        output_path = str(teacher_path).replace('.onnx', '_distilled.onnx')
    
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    logger.info("Knowledge Distillation (Simulated)")
    logger.info(f"Teacher: {teacher_path}")
    logger.info(f"Compression: {compression_ratio:.0%}")
    
    # For ONNX, we simulate distillation with aggressive quantization
    # Real distillation requires training data and PyTorch/TF
    from src.optimizers.quantizer import quantize_dynamic_int8
    
    result = quantize_dynamic_int8(teacher_path, output_path)
    
    logger.info(f"âœ“ Distilled model saved: {result}")
    return result
